---
title: "McGill University, BIOL 310 - Tutorial 2 - Supplementary material"
author: "Janaina Serrano"
date: "08/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Open spatial data

As supplementary analyses, we will explore some spatial datasets, both in vectorial and raster formats, to familiarize with R spatial; you can find more at <https://rspatial.org/>.

```{r, echo = TRUE}
# set up 
# packages needed in the tutorial
packages = c("data.table", "raster", "sp", "maptools", "dplyr","vegan")

# load the packages; if missing, install & load 
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)



```

We begin by opening a dataset of climatic velocity, approximated using the euclidean distance (i.e., shortest straight line) between two cells with analog climates. The dataset is available at <https://adaptwest.databasin.org/pages/adaptwest-velocitymed/>.

```{r, `echo = FALSE`}
# set up 

# open euclidead-distance based velocity; 
VEL_ED <- raster::raster("Data\\velocity\\velocity.ED.tif") #Velocity Euclidean Distance

VEL_ED 

plot(VEL_ED)


```

Two considerations: (i) the spatial grain of this raster is 5 km, and the coordinate system is a projected system in World Geodetic System 1984 (WGS84; <https://en.wikipedia.org/wiki/World_Geodetic_System>)

The plot shows up to 25 km/year migration needed to track future climates

```{r}
crs <- crs(VEL_ED)
crs
```

We will establish the coordinate reference systems (crs) from VEL_ED as the reference system in our analysis.

Next, let's create a raster of latitudinal values for all the cells present in VEL_ED

```{r}
# created a raster having the same extent of VEL_ED, but containing latitude values instead 

LAT <- VEL_ED # copy VEL_ED into a new object, LAT, which will 
raster_coordinates <- coordinates(VEL_ED) # extract the coordiantes of every cell
LAT[] <- raster_coordinates[, 2] # replace the content of every cell in LAT with its latitude

plot(LAT)

# remove pixels around NA for LAT
# create a mask
mask <- VEL_ED #create a mask based on VEL_ED
mask[!is.na(mask[])] <- 1 # every value in VEL_ED that is not NA becomes a 1
LAT <- LAT*mask # multiply the mask for LAT to remove all NAs in VEL_ED
plot(LAT)
```

# Aggregate and observe the results

```{r}
# aggregate to evaluate the same environmental gradients at different spatial scales
VEL_ED_50  <- raster::aggregate(VEL_ED, 10) 
VEL_ED_500 <- raster::aggregate(VEL_ED_50, 10)

LAT_50  <- raster::aggregate(LAT, 10)
LAT_500 <- raster::aggregate(LAT_50, 10)

# create unique ID cells for every 5, 50 and 500 km cell
ID <- VEL_ED
ID_50 <- VEL_ED_50
ID_500 <- VEL_ED_500

ID[] <- seq(1, ncell(VEL_ED))
ID_50[] <- seq(1, ncell(VEL_ED_50))
ID_500[] <- seq(1, ncell(VEL_ED_500))

# crop NA cells
ID <- ID*mask
ID_50 <- ID_50* (raster::aggregate(mask,10))
ID_500 <- ID_500*(raster::aggregate(mask,100))

# check visually the data
par(mfrow=c(3,3))
plot(VEL_ED)
plot(VEL_ED_50)
plot(VEL_ED_500)
plot(LAT)
plot(LAT_50)
plot(LAT_500)
plot(ID)
plot(ID_50)
plot(ID_500)

dev.off() # clean setting for 9 plots
```

\#Question: How does aggregation affects the covariates? How does aggregation affects the area of North America?

# Project the data downloaded from GBIF

```{r}
query_gbif <- data.table::fread("Data\\query_gbif_2013.csv", header = TRUE) # data downloaded from GBIF https://www.gbif.org/

```

```{r}
# project geographic coordinates (from degrees to metric units)
coordinates(query_gbif) <- c("decimalLongitude", "decimalLatitude")
proj4string(query_gbif) <- CRS("+init=epsg:4326") # GBIF geographic coordinates "+init=epsg:4326 +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"

query_gbif <- sp::spTransform(query_gbif, crs) # project according to adaptwest raster data
query_gbif <- as.data.frame(query_gbif)

# replace name of columns after projection
names(query_gbif)[names(query_gbif) == 'decimalLongitude'] <- 'x' # changed from degrees to metric
names(query_gbif)[names(query_gbif) == 'decimalLatitude'] <- 'y' # changed from degrees to metric

# prepare coordinates and data for spatial dataframe
coordinates_query_gbif <- query_gbif[ , c("x", "y")]   # coordinates; equivalent to query_gbif[, 49:50]
data_query_gbif   <- query_gbif[ , c(9:10, 31)]   # data you want to keep

# make a SpatialPointsDataFrame object
spdf <- sp::SpatialPointsDataFrame(coords       = coordinates_query_gbif,
                                    data        = data_query_gbif, 
                                    proj4string = crs)
```

# Check the distribution of checklists across Canada

Now that the occurrence data is projected, we can look at its distribution across Canada with more attention. First, we transform points into a raster, counting the number of occurrences in each 5 km cell. Then, we aggregate our spatial units to evaluate the degree to which community science efforts covered Canada.

```{r}
# look at how many cells have multiple observations
checklists <- raster::rasterize(coordinates_query_gbif, ID, fun='count')

plot(checklists) # difficult to see, 5 km cells are very small at the continental scale
par(mfrow=c(1,3))
plot(log10(checklists))
plot(log10(aggregate(checklists,10))) # 50 km resolution
plot(log10(aggregate(checklists,100))) # 500 km resolution
```

The effects of scaling on sampling coverage are substantial.

Last, we extract our covariates using the locations of occurrence of butterflies.

```{r}
# estract covariates
coordinates_query_gbif$extract_ID <- extract(ID, coordinates_query_gbif[, 1:2])
coordinates_query_gbif$extract_ID_50 <- extract(ID_50, coordinates_query_gbif[, 1:2])
coordinates_query_gbif$extract_ID_500 <- extract(ID_500, coordinates_query_gbif[, 1:2])

coordinates_query_gbif$extract_LAT <- extract(LAT, coordinates_query_gbif[, 1:2])
coordinates_query_gbif$extract_LAT_50 <- extract(LAT_50, coordinates_query_gbif[, 1:2])
coordinates_query_gbif$extract_LAT_500 <- extract(LAT_500, coordinates_query_gbif[, 1:2])

coordinates_query_gbif$extract_VEL_ED <- extract(VEL_ED, coordinates_query_gbif[, 1:2])
coordinates_query_gbif$extract_VEL_ED_50 <- extract(VEL_ED_50, coordinates_query_gbif[, 1:2])
coordinates_query_gbif$extract_VEL_ED_500 <- extract(VEL_ED_500, coordinates_query_gbif[, 1:2])

# # check relationship between LAT and VEL_ED across scales
# # might take a while; select the next three rows and press CTRL + SHIFT + C to run the code
par(mfrow=c(1,3))
plot(coordinates_query_gbif$extract_VEL_ED ~ coordinates_query_gbif$extract_LAT)
plot(coordinates_query_gbif$extract_VEL_ED_50 ~ coordinates_query_gbif$extract_LAT_50)
plot(coordinates_query_gbif$extract_VEL_ED_500 ~ coordinates_query_gbif$extract_LAT_500)

# create a unique coord field to merge with original data
coordinates_query_gbif$pasted_coord <- paste(coordinates_query_gbif$x, coordinates_query_gbif$y)
query_gbif$pasted_coord <- paste(query_gbif$x, query_gbif$y)
```

## Part 3: Spatiotemporal trends in biodiversity

I prepared a table where the 262 butterfly species of Canada are recorded in \~ 30.000 checklists between 2010 and 2021. For each checklist, I also provided spatial and temporal information (columns 262-280).

```{r}
final_table <- data.table::fread("Data\\final_table.csv", header = TRUE)
head(final_table$`Polygonia comma`)
head(final_table$extract_LAT)
head(final_table$year)
```

In the next lines, I created a script that (i) breaks the 30.000 rows table into multiple lists, each containing all the observations occurred at the same site. With "site" we used the "extract_ID" field representing each of the 5-km cell in which checklist were observed earlier. You can choose a different scale by replacing "extract_ID" with "extract_ID_50" or "extract_ID_500".

```{r}
## split into the 5-km cell communities
split_5 <- split(final_table, final_table$extract_ID)
# 4674 sites with at least one checklist


```

Then, I removed all sites with less than 10 checklists in 12 years

```{r}
# we keep only sites with at least 10 checklists
split_5 <- Filter(function(x) nrow(x) > 10, split_5) 
# 465 sites
```

Then, we retained only years wth at least 5 checklists, and removed sited with less than two years

```{r}
# how many transects per year
## split each cell in the 12 years
split_5 <- lapply(split_5, function(x) split(x, x$year))
for(i in 1:length(split_5)){
  split_5[[i]] <- Filter(function(x) nrow(x) > 5, split_5[[i]])
}

# remove sites with less than 2 years
split_5 <- Filter(function(x) length(x) > 2, split_5) # we assume that visits with only one observations were not checklists 
```

To account for differences in sampling effort, we resample the same number of checklists in each year in each site. We also remove sites with less than 3 years of data.

```{r}
## randomly select in each site an equal number of checklists
for(i in 1:length(split_5)){
  for(j in 1:length(split_5[[i]])){
    split_5[[i]][[j]] <- split_5[[i]][[j]][sample(nrow(split_5[[i]][[j]]), 5, replace = FALSE), ]
  }
}

# remove sites with only one or two years
split_5 <- Filter(function(x) length(x) > 2, split_5) # we assume that visits with only one observations were not checklists 

```

Now the data is ready to be analyzed. In the next section, we calculate species richness in each of the years for which we had at least 5 checklists, in each of the sites. I also calculated the average Jaccard similarity (<https://en.wikipedia.org/wiki/Jaccard_index>) across years in a site

```{r}
## calculate species richness
split_5_richness <- split_5
for(i in 1:length(split_5_richness)){
  for(j in 1:length(split_5_richness[[i]])){
    split_5_richness[[i]][[j]] <- sum(colSums(split_5[[i]][[j]][,1:262]) != 0) # the sum of all columns from column 1 to columns 262 that have at least one occurrence recorded
  }
}

for(i in 1:length(split_5_richness)){
  split_5_richness[[i]] <- do.call(rbind.data.frame, split_5_richness[[i]])
}

# community table
split_5_comm <- split_5
for(i in 1:length(split_5_comm)){
  for(j in 1:length(split_5_comm[[i]])){
    split_5_comm[[i]][[j]] <- as.data.frame(t(colSums(split_5_comm[[i]][[j]][,1:262])))
    split_5_comm[[i]][[j]][split_5_comm[[i]][[j]] > 0] <- 1
  }
}

for(i in 1:length(split_5_comm)){
  split_5_comm[[i]] <- do.call(rbind.data.frame, split_5_comm[[i]])
}

# calculate the mean Jaccard similarity across the species found in a site in different years 
split_5_similarity <- split_5_comm
for(i in 1:length(split_5_similarity)){
  split_5_similarity[[i]] <- mean(vegdist(split_5_similarity[[i]], method = "jaccard"))
}

## years relative to each list
years <- list()
for(i in 1: length(split_5)){
years[[i]] <-  as.numeric(names(split_5[[i]])) 
}

```

Last, we fit some models predicting trends in species richness at each site

```{r}

# trends in richness
models <- list()
for(i in 1:length(years)){
  models[[i]] <- lm(split_5_richness[[i]][,1] ~ years[[i]]) # model where richness at a site is predicted as a function of time
  models[[i]] <- models[[i]]$coefficients[2] # keep only the year effect
}

# for(i in 1:length(years)){
#   models[[i]] <- models[[i]]$coefficients[2]
# }

# velocity
split_5_velocity <- split_5

for(i in 1:length(split_5_velocity)){
    split_5_velocity[[i]] <- split_5_velocity[[i]][[1]]$extract_VEL_ED[[1]]
  }

split_5_latitude <- split_5

for(i in 1:length(split_5_latitude)){
  split_5_latitude[[i]] <- split_5_latitude[[i]][[1]]$extract_LAT[[1]]
}


# clean the datasets
models <- do.call(rbind.data.frame, models)
colnames(models)[1] <- "slope"
similarity <-do.call(rbind.data.frame, split_5_similarity)
colnames(similarity)[1] <- "similarity"
velocity <- do.call(rbind.data.frame, split_5_velocity)
colnames(velocity)[1] <- "velocity"
latitude <- do.call(rbind.data.frame, split_5_latitude)
colnames(latitude)[1] <- "latitude"

summary(lm(models$slope ~ latitude$latitude))

# # check other relationsips
plot(models$slope ~ velocity$velocity)

plot(similarity$similarity ~ latitude$latitude)
plot(models$slope ~ similarity$similarity)
```

The relationship between richness changes at a site ("slope") and latitude tend to be positive, and at time significant. Because the resampling of checklist in every site is stochastic, you will find results different from your classmates.

```{r}

#Create a function to generate a continuous color palette
rbPal <- colorRampPalette(c('blue','red'))

#This adds a column of color values
# based on the y values
models$Col <- rbPal(10)[as.numeric(cut(models$slope,breaks = 10))]
plot(models$slope ~ latitude$latitude,  
     col = models$Col, 
     pch = 16,
     main="Relationship between biodiversity trends and latitude", 
     xlab="Latitude (m)", ylab="slope of the richness ~ year model")
```

You can see positive relationships between richness and time in red, and negative in blue. It seems like many sites are characterized by a loss of diversity, except for a few northern sites that seem to have positive diversity trends.
